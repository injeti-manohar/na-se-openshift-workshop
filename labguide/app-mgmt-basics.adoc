## Application Management Basics
In this module, you will deploy a sample microservices application using the
`oc` tool and learn about some of the core concepts, fundamental objects, and
basics of application management on OpenShift Container Platform.

You will be deploying the CoolStore demo application which is comprised of 4
microservices.  The frontend is written in NodeJS which talks to an API Gateway.

The API Gateway is created using a reactive framework called https://vertx.io[Vertx],
which is a toolkit for building reactive applications on the Java Virtual Machine (JVM).
Vert.x does not impose a specific framework or packaging model and can be used
within your existing applications and frameworks in order to add reactive
functionality by just adding the Vert.x jar files to the application classpath.

Our API Gateway manages connections to our Inventory and Catalog backend services.
The Catalog was created using SpringBoot and Inventory is a Java application
packaged with WildFly Swarm.  WildFly Swarm offers an innovative approach to
packaging and running Java EE applications by packaging them with just enough of
the Java EE server runtime to be able to run them directly on the JVM using java
-jar. For more details on various approaches to packaging Java applications,
https://developers.redhat.com/blog/2017/08/24/the-skinny-on-fat-thin-hollow-and-uber[read this blog post].

[NOTE]
====
You will want to have an {{ SSH_CONSOLE_URL }}[SSH] session opened to the `master` server for these
lab exercises.
====

The first step for any deployment pipeline is to store all code and configurations in
a source code repository.

In order to get started, you need to download the project source code.

[source]
----
mkdir cloud-native-labs
cd cloud-native-labs
curl -skL -o projects.tar.gz https://github.com/sa-ne/cloud-native-labs/archive/ocp-3.10.tar.gz
tar xzvf projects.tar.gz
cd cloud-native-labs-ocp-3.10/
----

You should see these folders.

[source]
----
$ ls -l

-rwxr-xr-x  1 user  wheel  1718 Aug 14 14:50 README.md
drwxr-xr-x  6 user  wheel   204 Aug 14 14:50 catalog-spring-boot
drwxr-xr-x  6 user  wheel   204 Aug 14 14:50 gateway-vertx
drwxr-xr-x  6 user  wheel   204 Aug 14 14:50 inventory-wildfly-swarm
drwxr-xr-x  9 user  wheel   306 Aug 14 14:50 solutions
drwxr-xr-x  8 user  wheel   272 Aug 14 14:50 web-nodejs
----

You will use https://github.com/[GitHub], the vastly popular web-based Git hosting for this
lab. If you don't already have an account on GitHub, you really should now! Head to
https://github.com/[GitHub] and sign in with your account or sign up for a new account.

[NOTE]
====
You could use any Git hosting such as GitLab, BitBucket, etc. However the
instructions in this lab assume you are using GitHub.
====

Click on the plus icon on the top navigation bar and then on *New Repository*.

image::cd-github-plus-icon.png[]

Give `cloud-native-labs` as **Repository name** and click on **Create repository**
button, leaving the rest with default values.

image::cd-github-new-repo.png[]

The Git repository is created now. Click on the copy-to-clipboard icon to near the
HTTPS Git url to copy it to the clipboard which you will need in a few minutes.

image::cd-github-empty-repo.png[]

Before you commit the source code to the Git repository, configure your name and
email so that the commit owner can be seen on the repository. If you want, you can
replace the name and the email with your own in the following commands:

[source]
----
git config --global user.name "Developer"
git config --global user.email "developer@me.com"
----

Next, we'll initialize the local git repository and push it to your new remote
repository

[NOTE]
====
In subsequent commands, replace `GIT-REPO-URL` with the Git repository url
copied in the previous steps
====

[source]
----
git init
git remote add origin GIT-REPO-URL

git add . --all
git commit -m "initial add"
git push -u origin master
----

Enter your Git repository username and password if you get asked to enter your
credentials. This may require you to provision a developer token in GitHub.
To do this, from the settings of your GitHub profile, navigate to
*Developer Settings >> Personal Access Tokens* and click *Generate new token*.
Use this generated token as your password.

Go to your `cloud-native-labs` repository web interface and refresh
the page. You should see the project files in the repository.

### Core OpenShift Concepts
As a future administrator of OpenShift, it is important to understand several
core building blocks as it relates to applications. Understanding these building
blocks will help you better see the big picture of application management on the
platform.

#### Projects
A *Project* is a "bucket" of sorts. It's a meta construct where all of a user's
resources live. From an administrative perspective, each *Project* can be
thought of like a tenant. *Projects* may have multiple users who can access
them, and users may be able to access multiple *Projects*.

For this exercise, first create a *Project* to hold our resources:

[source,bash]
----
oc login -u teamuser1 -p openshift
oc new-project coolstore
----

#### Deploy Microservices
The `new-app` command provides a very simple way to tell OpenShift to run
things. You simply provide it with one of a wide array of inputs, and it figures
out what to do. Users will commonly use this command to get OpenShift to launch
existing images, to create builds of source code and ultimately deploy them, to
instantiate templates, and so on.

The first microservice we'll launch is our Inventory service. This service has
been built using WildFly Swarm.  Wildfly Swarm offers an innovative approach to
packaging and running Java EE applications by packaging them with just enough of
the Java EE server runtime to be able to run them directly on the JVM using
java -jar

To easily launch this service, we will run an S2I build.  OpenShift S2I uses the
supported OpenJDK container image to build the final container image of the
Inventory service by uploading the WildFly Swam uber-jar from the target folder
to the OpenShift platform.

[source,bash]
----
oc new-app redhat-openjdk18-openshift:1.2~. \
    --context-dir=inventory-wildfly-swarm \
    --name=inventory
----

The output will look like:

----
--> Found image 56cfa0a (7 months old) in image stream "openshift/redhat-openjdk18-openshift" under tag "1.2" for "redhat-openjdk18-openshift:1.2"

    Java Applications
    -----------------
    Platform for building and running plain Java applications (fat-jar and flat classpath)

    Tags: builder, java

    * A source build using source code from https://github.com/openshift-labs/cloud-native-labs.git#ocp-3.10 will be created
      * The resulting image will be pushed to image stream "inventory:latest"
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config "inventory"
    * Ports 8080/tcp, 8443/tcp, 8778/tcp will be load balanced by service "inventory"
      * Other containers can access this service through the hostname "inventory"

--> Creating resources ...
    imagestream "inventory" created
    buildconfig "inventory" created
    deploymentconfig "inventory" created
    service "inventory" created
--> Success
    Build scheduled, use 'oc logs -f bc/inventory' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/inventory'
    Run 'oc status' to view your app.
----

You can see that OpenShift automatically created several resources as the output
of this command. As part of the deployment, OpenShift is running a build for us
by fetching the source code from our github repository and running build scripts
that are included in our S2I builder image.  This process will take a few minutes
to complete.  You can monitor the status either through the Web UI or through
the command line:

[source,bash]
----
oc logs -f bc/inventory
----

We will take some time to explore the resources that were
created.

For more information on the capabilities of `new-app`, take a look at its help
message by running `oc new-app -h`.

#### Pods

.OpenShift Pods
image::openshift_pod.png[]

Pods are one or more containers deployed together on host. A pod is the
smallest compute unit you can define, deploy and manage. Each pod is allocated
its own internal IP address on the SDN and will own the entire port range. The
containers within pods can share local storage space and networking resources.

Pods are treated as **static** objects by OpenShift, i.e., one cannot change the
pod definition while running.

You can get a list of pods:

[source,bash]
----
oc get pods
----

And you will see something like the following:

----
NAME                READY     STATUS      RESTARTS   AGE
inventory-1-build   0/1       Completed   0          4m
inventory-1-88p54   1/1       Running     0          13m
----

NOTE: Pod names are dynamically generated as part of the deployment process,
which you will learn about shortly. Your name will be slightly different.

The `describe` command will give you more information on the details of a pod.
In the case of the pod name above:

[source,bash]
----
oc describe pod inventory-1-88p54
----

And you will see output similar to the following:

----
Name:           inventory-1-88p54
Namespace:      coolstore
Node:           ip-10-0-0-50.ca-central-1.compute.internal/10.0.0.50
Start Time:     Tue, 28 Aug 2018 19:38:08 -0400
Labels:         app=inventory
                deployment=inventory-1
                deploymentconfig=inventory
Annotations:    openshift.io/deployment-config.latest-version=1
                openshift.io/deployment-config.name=inventory
                openshift.io/deployment.name=inventory-1
                openshift.io/generated-by=OpenShiftNewApp
                openshift.io/scc=restricted
Status:         Running
IP:             10.1.5.87
Controlled By:  ReplicationController/inventory-1
Containers:
  inventory:
    Container ID:   docker://be1871d2a65d3a5d148c8643a2bb4428415a14f288a23f1eb1caa1f6e0cb2042
    Image:          docker-registry.default.svc:5000/coolstore/inventory@sha256:07778ae03893fa34eeccf63d91d830cc171a50734cecf49e8e33dd6cefd545bb
    Image ID:       docker-pullable://docker-registry.default.svc:5000/coolstore/inventory@sha256:07778ae03893fa34eeccf63d91d830cc171a50734cecf49e8e33dd6cefd545bb
    Ports:          8080/TCP, 8443/TCP, 8778/TCP
    Host Ports:     0/TCP, 0/TCP, 0/TCP
    State:          Running
      Started:      Tue, 28 Aug 2018 19:38:10 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7qpj2 (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          True
  PodScheduled   True
Volumes:
  default-token-7qpj2:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-7qpj2
...
----

This is a more detailed description of the pod that is running. You can see what
node the pod is running on, the internal IP address of the pod, various labels,
and other information about what is going on.

#### Services
.OpenShift Service
image::openshift_service.png[]

*Services* provide a convenient abstraction layer inside OpenShift to find a
group of like *Pods*. They also act as an internal proxy/load balancer between
those *Pods* and anything else that needs to access them from inside the
OpenShift environment. For example, if you needed more `inventory` instances to
handle the load, you could spin up more *Pods*. OpenShift automatically maps
them as endpoints to the *Service*, and the incoming requests would not notice
anything different except that the *Service* was now doing a better job handling
the requests.

When you asked OpenShift to run the image, it automatically created a *Service*
for you. Remember that services are an internal construct. They are not
available to the "outside world", or anything that is outside the OpenShift
environment. That's OK, as you will learn later.

The way that a *Service* maps to a set of *Pods* is via a system of *Labels* and
*Selectors*. *Services* are assigned a fixed IP address and many ports and
protocols can be mapped.

There is a lot more information about
https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/pods_and_services.html#services[Services],
including the YAML format to make one by hand, in the official documentation.

The `new-app` command used earlier caused a service to be created. You can see
the current list of services in a project with:

[source,bash]
----
oc get services
----

You will see something like the following:

----
NAME      CLUSTER-IP     EXTERNAL-IP     PORT(S)                      AGE
inventory 172.30.87.247    <none>        8080/TCP,8443/TCP,8778/TCP   19m
----

NOTE: Service IP addresses are dynamically assigned on creation and are
immutable. The IP of a service will never change, and the IP is reserved until
the service is deleted. Your service IP will likely be different.

Just like with pods, you can `describe` services, too. In fact, you can
`describe` most objects in OpenShift:

[source,bash]
----
oc describe service inventory
----

You will see something like the following:

----
Name:              inventory
Namespace:         coolstore
Labels:            app=inventory
Annotations:       openshift.io/generated-by=OpenShiftNewApp
Selector:          app=inventory,deploymentconfig=inventory
Type:              ClusterIP
IP:                172.30.87.247
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.1.5.87:8080
Port:              8443-tcp  8443/TCP
TargetPort:        8443/TCP
Endpoints:         10.1.5.87:8443
Port:              8778-tcp  8778/TCP
TargetPort:        8778/TCP
Endpoints:         10.1.5.87:8778
Session Affinity:  None
Events:            <none>
----

Information about all objects (their definition, their state, and so forth) is
stored in the etcd datastore. etcd stores data as key/value pairs, and all of
this data can be represented as serializable data objects (JSON, YAML).

Take a look at the YAML output for the service:

[source,bash]
----
oc get service inventory -o yaml
----

You will see something like the following:

----
apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2018-08-28T23:34:21Z
  labels:
    app: inventory
  name: inventory
  namespace: coolstore
  resourceVersion: "14790494"
  selfLink: /api/v1/namespaces/coolstore/services/inventory
  uid: e39c2e05-ab1a-11e8-9d47-021570a77a16
spec:
  clusterIP: 172.30.87.247
  ports:
  - name: 8080-tcp
    port: 8080
    protocol: TCP
    targetPort: 8080
  - name: 8443-tcp
    port: 8443
    protocol: TCP
    targetPort: 8443
  - name: 8778-tcp
    port: 8778
    protocol: TCP
    targetPort: 8778
  selector:
    app: inventory
    deploymentconfig: inventory
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
----

Take note of the `selector` stanza. Remember it.

It is also of interest to view the YAML of the *Pod* to understand how OpenShift
wires components together. Go back and find the name of your `inventory` *Pod*,
and then execute the following:

[source,bash]
----
oc get pod inventory-1-88p54 -o yaml
----

Under the `metadata` section you should see the following:

----
  labels:
    app: inventory
    deployment: inventory-1
    deploymentconfig: inventory
  name: inventory-1-88p54
----

* The *Service* has `selector` stanza that refers to `app: inventory` and
  `deploymentconfig: inventory`.
* The *Pod* has multiple *Labels*:
** `deploymentconfig: inventory`
** `app: inventory`
** `deployment: inventory-1`

*Labels* are just key/value pairs. Any *Pod* in this *Project* that has a *Label* that
matches the *Selector* will be associated with the *Service*. If you look at the
`describe` output again, you will see that there is one endpoint for the
service: the existing `inventory` *Pod*.

The default behavior of `new-app` is to create just one instance of the item
requested. We will see how to modify/adjust this in a moment, but there are a
few more concepts to learn first.

### Background: Deployment Configurations and Replication Controllers

While *Services* provide routing and load balancing for *Pods*, which may go in
and out of existence, *ReplicationControllers* (RC) are used to specify and then
ensure the desired number of *Pods* (replicas) are in existence. For example, if
you always want an application to be scaled to 3 *Pods* (instances), a
*ReplicationController* is needed. Without an RC, any *Pods* that are killed or
somehow die/exit are not automatically restarted. *ReplicationControllers* are
how OpenShift "self heals".

A *DeploymentConfiguration* (DC) defines how something in OpenShift should be
deployed. From the https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/deployments.html[deployments documentation^]:

----
Building on replication controllers, OpenShift adds expanded support for the
software development and deployment lifecycle with the concept of deployments.
In the simplest case, a deployment just creates a new replication controller and
lets it start up pods. However, OpenShift deployments also provide the ability
to transition from an existing deployment of an image to a new one and also
define hooks to be run before or after creating the replication controller.
----

In almost all cases, you will end up using the *Pod*, *Service*,
*ReplicationController* and *DeploymentConfiguration* resources together. And, in
almost all of those cases, OpenShift will create all of them for you.

There are some edge cases where you might want some *Pods* and an *RC* without a *DC*
or a *Service*, and others, but these are advanced topics not covered in these
exercises.

#### Exploring Deployment-related Objects

Now that we know the background of what a *ReplicatonController* and
*DeploymentConfig* are, we can explore how they work and are related. Take a
look at the *DeploymentConfig* (DC) that was created for you when you told
OpenShift to stand up the `inventory` service:

[source,bash]
----
oc get dc
----

You will see something like the following:

----
NAME          REVISION   DESIRED   CURRENT   TRIGGERED BY
inventory     1          1         1         config,image(inventory:latest)
----

To get more details, we can look into the *ReplicationController* (*RC*).

Take a look at the *ReplicationController* (RC) that was created for you when
you told OpenShift to stand up the `inventory` service:

[source,bash]
----
oc get rc
----

You will see something like the following:

----
NAME          DESIRED   CURRENT   READY     AGE
inventory-1   1         1         1         4h
----

This lets us know that, right now, we expect one *Pod* to be deployed
(`Desired`), and we have one *Pod* actually deployed (`Current`). By changing
the desired number, we can tell OpenShift that we want more or less *Pods*.

In a future lab, we'll explore how replication controllers help us scale our applications.

One last thing to note is that there are actually several ports defined on this
*Service*. Earlier we said that a pod gets a single IP and has control of the
entire port space on that IP. While something running inside the *Pod* may listen
on multiple ports (single container using multiple ports, individual containers
using individual ports, a mix), a *Service* can actually proxy/map ports to
different places.

For example, a *Service* could listen on port 80 (for legacy reasons) but the
*Pod* could be listening on port 8080, 8888, or anything else.

### Deploy Remaining Microservices

Let's go ahead and deploy the rest of our microservices.  We're going to deploy
the remaining services: Catalog, Gateway, and Web UI.

The Catalog service is a spring boot application.  We will, once again, use the
Open JDK S2I image.

[source,bash]
----
oc new-app redhat-openjdk18-openshift:1.2~. \
  --context-dir=catalog-spring-boot \
  --name=catalog
----

The Gateway service is an Eclipse Vert.x application which will also use the
Open JDK S2I image.  Vert.x service discovery integrates into OpenShift service
discovery via OpenShift REST API and imports available services to make them
available to the Vert.x application.  Every pod in OpenShift runs using a
service account.  By default, all pods use the `default` service account.  In
order for the `gateway` service to use OpenShift's service discovery, we need to
grant the `default` service account the `view` role in this project:

[source,bash]
----
oc policy add-role-to-user view -z default
----

[source,bash]
----
oc new-app redhat-openjdk18-openshift:1.2~. \
  --context-dir=gateway-vertx \
  --name=gateway
----


Lastly, we need to deploy our Web UI which will use the node.js S2I builder image.

[source,bash]
----
oc new-app nodejs:8~. \
  --context-dir=web-nodejs \
  --name=web
----

Lets use the OpenShift Web Console to monitor the rest of our deployments:

*link:{{ WEB_CONSOLE_URL }}/project/coolstore/overview[]*

[WARNING]
====
If you see the below error regarding metrics, click on the *Open Metrics URL*
link and accept the invalid certificate.  Navigate back to the OpenShift Web
Console and refresh the page.

image::metrics_error.png[]
====

### Background: Routes
.OpenShift Route
image::openshift_route.png[]

While *Services* provide internal abstraction and load balancing within an
OpenShift environment, sometimes clients (users, systems, devices, etc.)
**outside** of OpenShift need to access an application. The way that external
clients are able to access applications running in OpenShift is through the
OpenShift routing layer. And the data object behind that is a *Route*.

The default OpenShift router (HAProxy) uses the HTTP header of the incoming
request to determine where to proxy the connection. You can optionally define
security, such as TLS, for the *Route*. If you want your *Services* (and by
extension, your *Pods*) to be accessible to the outside world, then you need to
create a *Route*.

Do you remember setting up the router? You probably don't. That's because the
installer settings created a router for you! The router lives in the `default`
*Project*, and you can see information about it with the following command:

[source,bash]
----
oc describe dc router -n default
----

#### Creating a Route
Creating a *Route* is a pretty straight-forward process.  You simply `expose`
the *Service* via the command line. With the *Service* name, creating a *Route* is a simple
one-command task.  Create routes for each of your services:

[source,bash]
----
oc expose service web
oc expose service gateway
oc expose service inventory
oc expose service catalog
----

You will see:

----
route "web" exposed
----

Verify the *Route* was created with the following command:

[source,bash]
----
oc get route
----

You will see something like:

----
NAME        HOST/PORT                                                     PATH      SERVICES   PORT       TERMINATION   WILDCARD
catalog     catalog-coolstore.{{OCP_ROUTING_SUFFIX}}             catalog     8080-tcp                 None
gateway     gateway-coolstore.{{OCP_ROUTING_SUFFIX}}             gateway     8080-tcp                 None
inventory   inventory-coolstore.{{OCP_ROUTING_SUFFIX}}           inventory   8080-tcp                 None
web         web-coolstore.{{OCP_ROUTING_SUFFIX}}                 web         8080-tcp                 None
----

If you take a look at the `HOST/PORT` column, you'll see a familiar looking
FQDN. The default behavior of OpenShift is to expose services on a formulaic
hostname:

`{SERVICENAME}-{PROJECTNAME}.{ROUTINGSUBDOMAIN}`

How does this work? Firstly, the `ROUTINGSUBDOMAIN` can be configured at install
time. We did this for you. In the `/etc/ansible/hosts` file you will find the
following line:

[source,yaml]
----
openshift_master_default_subdomain={{OCP_ROUTING_SUFFIX}}
----

There is also a wildcard DNS entry that points `+*.apps...+` to the host where the
router lives. OpenShift concatenates the *Service* name, *Project* name, and the
routing subdomain to create this FQDN/URL.

You can visit this URL using your browser, or using `curl`, or any other tool.
It should be accessible from anywhere on the internet.

The *Route* is associated with the *Service*, and the router automatically
proxies connections directly to the *Pod*. The router itself runs as a *Pod*. It
bridges the "real" internet to the SDN.

At this point our application should be deployed and accessible through the
Web UI Route:

*link:http://web-coolstore.{{OCP_ROUTING_SUFFIX}}[]*

If you take a step back to examine everything you've done so far, in just a few
commands, you deployed an application composed of several microservices and made
it accessible to the outside world, all without writing a single Dockerfile.

----
# Create Project
oc new-project coolstore

# Setup Permissions
oc policy add-role-to-user view -z default

# Deploy Services
oc new-app redhat-openjdk18-openshift:1.2~. \
    --context-dir=inventory-wildfly-swarm \
    --name=inventory

oc new-app redhat-openjdk18-openshift:1.2~. \
  --context-dir=catalog-spring-boot \
  --name=catalog

oc new-app redhat-openjdk18-openshift:1.2~. \
  --context-dir=gateway-vertx \
  --name=gateway

oc new-app nodejs:8~. \
  --context-dir=web-nodejs \
  --name=web

# Expose Services
oc expose service web
oc expose service gateway
oc expose service inventory
oc expose service catalog
----
